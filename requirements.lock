# generated by rye
# use `rye lock` or `rye sync` to update this lockfile
#
# last locked with the following flags:
#   pre: false
#   features: []
#   all-features: false
#   with-sources: false
#   generate-hashes: false
#   universal: false

-e file:.
annotated-types==0.7.0
    # via pydantic
anyio==4.10.0
    # via httpx
certifi==2025.8.3
    # via httpcore
    # via httpx
    # via requests
charset-normalizer==3.4.3
    # via requests
coloredlogs==15.0.1
    # via onnxruntime
fastembed==0.7.3
    # via knowledge-bot-telegram
filelock==3.19.1
    # via huggingface-hub
    # via torch
    # via transformers
flatbuffers==25.2.10
    # via onnxruntime
fsspec==2025.9.0
    # via huggingface-hub
    # via torch
grpcio==1.75.0
    # via qdrant-client
h11==0.16.0
    # via httpcore
h2==4.3.0
    # via httpx
hf-xet==1.1.10
    # via huggingface-hub
hpack==4.1.0
    # via h2
httpcore==1.0.9
    # via httpx
httpx==0.28.1
    # via knowledge-bot-telegram
    # via python-telegram-bot
    # via qdrant-client
huggingface-hub==0.35.0
    # via fastembed
    # via sentence-transformers
    # via tokenizers
    # via transformers
humanfriendly==10.0
    # via coloredlogs
hyperframe==6.1.0
    # via h2
idna==3.10
    # via anyio
    # via httpx
    # via requests
jinja2==3.1.6
    # via torch
joblib==1.5.2
    # via scikit-learn
loguru==0.7.3
    # via fastembed
markupsafe==3.0.2
    # via jinja2
mmh3==5.2.0
    # via fastembed
mpmath==1.3.0
    # via sympy
networkx==3.5
    # via torch
numpy==2.3.3
    # via fastembed
    # via onnxruntime
    # via qdrant-client
    # via scikit-learn
    # via scipy
    # via transformers
nvidia-cublas-cu12==12.8.4.1
    # via nvidia-cudnn-cu12
    # via nvidia-cusolver-cu12
    # via torch
nvidia-cuda-cupti-cu12==12.8.90
    # via torch
nvidia-cuda-nvrtc-cu12==12.8.93
    # via torch
nvidia-cuda-runtime-cu12==12.8.90
    # via torch
nvidia-cudnn-cu12==9.10.2.21
    # via torch
nvidia-cufft-cu12==11.3.3.83
    # via torch
nvidia-cufile-cu12==1.13.1.3
    # via torch
nvidia-curand-cu12==10.3.9.90
    # via torch
nvidia-cusolver-cu12==11.7.3.90
    # via torch
nvidia-cusparse-cu12==12.5.8.93
    # via nvidia-cusolver-cu12
    # via torch
nvidia-cusparselt-cu12==0.7.1
    # via torch
nvidia-nccl-cu12==2.27.3
    # via torch
nvidia-nvjitlink-cu12==12.8.93
    # via nvidia-cufft-cu12
    # via nvidia-cusolver-cu12
    # via nvidia-cusparse-cu12
    # via torch
nvidia-nvtx-cu12==12.8.90
    # via torch
onnxruntime==1.22.1
    # via fastembed
packaging==25.0
    # via huggingface-hub
    # via onnxruntime
    # via transformers
pillow==11.3.0
    # via fastembed
    # via sentence-transformers
portalocker==3.2.0
    # via qdrant-client
protobuf==6.32.1
    # via onnxruntime
    # via qdrant-client
py-rust-stemmers==0.1.5
    # via fastembed
pydantic==2.11.9
    # via knowledge-bot-telegram
    # via qdrant-client
pydantic-core==2.33.2
    # via pydantic
pymupdf==1.26.4
    # via pymupdf4llm
pymupdf4llm==0.0.27
    # via knowledge-bot-telegram
python-dotenv==1.1.1
    # via knowledge-bot-telegram
python-telegram-bot==22.4
    # via knowledge-bot-telegram
pyyaml==6.0.2
    # via huggingface-hub
    # via knowledge-bot-telegram
    # via transformers
qdrant-client==1.15.1
    # via knowledge-bot-telegram
regex==2025.9.18
    # via tiktoken
    # via transformers
requests==2.32.5
    # via fastembed
    # via huggingface-hub
    # via tiktoken
    # via transformers
safetensors==0.6.2
    # via transformers
scikit-learn==1.7.2
    # via sentence-transformers
scipy==1.16.2
    # via scikit-learn
    # via sentence-transformers
sentence-transformers==5.1.0
    # via knowledge-bot-telegram
setuptools==80.9.0
    # via torch
    # via triton
sniffio==1.3.1
    # via anyio
sympy==1.14.0
    # via onnxruntime
    # via torch
threadpoolctl==3.6.0
    # via scikit-learn
tiktoken==0.11.0
    # via knowledge-bot-telegram
tokenizers==0.22.1
    # via fastembed
    # via transformers
torch==2.8.0
    # via sentence-transformers
tqdm==4.67.1
    # via fastembed
    # via huggingface-hub
    # via sentence-transformers
    # via transformers
transformers==4.56.2
    # via sentence-transformers
triton==3.4.0
    # via torch
typing-extensions==4.15.0
    # via anyio
    # via grpcio
    # via huggingface-hub
    # via pydantic
    # via pydantic-core
    # via sentence-transformers
    # via torch
    # via typing-inspection
typing-inspection==0.4.1
    # via pydantic
urllib3==2.5.0
    # via qdrant-client
    # via requests
